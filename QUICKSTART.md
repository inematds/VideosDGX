# Guia de In√≠cio R√°pido - VideosDGX

Guia passo-a-passo para come√ßar a usar os modelos de gera√ß√£o de v√≠deo.

## üéØ Objetivo

Ter os 4 modelos rodando e gerando v√≠deos em menos de 30 minutos.

## üìã Pr√©-requisitos

Antes de come√ßar, verifique:

```bash
# Docker instalado (vers√£o 24.0+)
docker --version

# Docker Compose instalado (vers√£o 2.20+)
docker-compose --version

# NVIDIA Docker Runtime
docker run --rm --gpus all nvidia/cuda:12.3.0-base-ubuntu22.04 nvidia-smi

# Espa√ßo em disco (pelo menos 100GB livre)
df -h
```

Se algum comando falhar, consulte a se√ß√£o **Instala√ß√£o de Depend√™ncias** no final.

## üöÄ Passo 1: Build da Imagem Base

A imagem base cont√©m CUDA, PyTorch e todas as depend√™ncias comuns.

```bash
cd /caminho/para/VideosDGX

# Build (leva ~10-15 minutos)
docker build -t videosdgx-base:latest -f common/base.Dockerfile .
```

Ou use o Makefile:

```bash
make base
```

**Verifica√ß√£o**:
```bash
docker images | grep videosdgx-base
# Deve mostrar: videosdgx-base  latest  ...  ~15GB
```

## üöÄ Passo 2: Download dos Modelos

‚ö†Ô∏è **IMPORTANTE**: Esta √© uma simula√ß√£o. Os IDs de modelos usados s√£o hipot√©ticos. Em produ√ß√£o, substitua pelos IDs reais do HuggingFace.

### Op√ß√£o A: Script Interativo (Recomendado)

```bash
./scripts/download_models.sh
```

Escolha a op√ß√£o 5 para baixar todos os modelos.

### Op√ß√£o B: Manual

```bash
# Criar volumes
docker volume create videosdgx_models
docker volume create videosdgx_outputs

# Para cada modelo, baixar do HuggingFace
# Exemplo para LTX-2:
huggingface-cli download Lightricks/LTX-Video \
  --local-dir /var/lib/docker/volumes/videosdgx_models/_data/ltx2
```

**Verifica√ß√£o**:
```bash
docker volume ls | grep videosdgx
# Deve mostrar:
# videosdgx_models
# videosdgx_outputs
```

## üöÄ Passo 3: Build dos Containers

Agora construa os containers espec√≠ficos de cada modelo:

```bash
docker-compose build
```

Ou com Makefile:

```bash
make build
```

Isso levar√° ~20-30 minutos dependendo da conex√£o. As imagens finais ter√£o ~20-25GB cada.

**Verifica√ß√£o**:
```bash
docker images | grep videosdgx
# Deve mostrar:
# videosdgx-base
# videosdgx-ltx2
# videosdgx-wan21
# videosdgx-magi1
# videosdgx-waver
```

## üöÄ Passo 4: Iniciar os Servi√ßos

```bash
docker-compose up -d
```

Ou:

```bash
make up
```

Os containers v√£o iniciar em background. Aguarde ~30 segundos para as APIs estarem prontas.

**Verifica√ß√£o**:
```bash
docker-compose ps
# Todos devem estar "Up" e "healthy"

# Ou use o health check script
./scripts/health_check.py
```

Sa√≠da esperada:
```
=========================================
VideosDGX - Health Check
=========================================

‚óè LTX-2
   Status:      Online
   Endpoint:    http://localhost:8001
   Modelo:      N√£o carregado
   ...

‚úì Todos os servi√ßos est√£o saud√°veis
```

## üé¨ Passo 5: Gerar Seu Primeiro V√≠deo

Agora vamos testar gerando um v√≠deo simples com o Waver (modelo mais r√°pido):

```bash
# Gerar v√≠deo
curl -X POST http://localhost:8004/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "A cat walking on a beach at sunset",
    "duration": 3,
    "resolution": "512x512",
    "fps": 24
  }'
```

Resposta (exemplo):
```json
{
  "job_id": "waver-abc12345",
  "status": "queued",
  "queue_position": 1,
  "estimated_time_seconds": 60,
  "model_loaded": false
}
```

**Importante**: Na primeira execu√ß√£o, o modelo ser√° carregado automaticamente, o que pode levar 40-60 segundos adicionais.

### Verificar Status

```bash
# Substitua pelo seu job_id
curl http://localhost:8004/jobs/waver-abc12345
```

Aguarde at√© `"status": "completed"`.

### Download do V√≠deo

```bash
# Via curl
curl -O http://localhost:8004/jobs/waver-abc12345/download

# Ou abra no navegador
# http://localhost:8004/jobs/waver-abc12345/download
```

O v√≠deo ser√° salvo como `download` (renomeie para `.mp4`).

## üé® Testando Outros Modelos

Agora que tudo est√° funcionando, teste os outros modelos:

### LTX-2 (Full video + audio) - Porta 8001

```bash
curl -X POST http://localhost:8001/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "A cinematic shot of a city at night with rain",
    "duration": 5,
    "resolution": "1024x576",
    "fps": 24,
    "guidance_scale": 7.5
  }'
```

### Wan 2.1 (Versatile) - Porta 8002

```bash
curl -X POST http://localhost:8002/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Timelapse of clouds moving over mountains",
    "duration": 4,
    "resolution": "1024x576",
    "fps": 30
  }'
```

### MAGI-1 (Long-form) - Porta 8003

```bash
curl -X POST http://localhost:8003/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "A story of a day in the life of a bird",
    "duration": 10,
    "resolution": "1024x576",
    "fps": 24
  }'
```

## üìä Monitoramento

### Health Check Completo

```bash
./scripts/health_check.py
```

### Benchmark de Performance

```bash
# Teste completo
./scripts/benchmark.py

# Teste r√°pido
./scripts/benchmark.py --quick

# Apenas um modelo
./scripts/benchmark.py --model waver
```

### Ver Logs

```bash
# Todos os containers
docker-compose logs -f

# Container espec√≠fico
docker-compose logs -f ltx2

# √öltimas 50 linhas
docker-compose logs --tail=50 wan21
```

### Uso de GPU

```bash
nvidia-smi

# Ou com watch (atualiza a cada 1s)
watch -n 1 nvidia-smi
```

## üîß Comandos √öteis

### Parar Todos os Servi√ßos

```bash
docker-compose down
# ou
make down
```

### Reiniciar um Container

```bash
docker-compose restart ltx2
```

### Ver Status dos Containers

```bash
docker-compose ps
```

### Ver Uso de Recursos

```bash
docker stats
```

### Descarregar Modelo da Mem√≥ria

```bash
# Libera ~25-30GB de mem√≥ria
curl -X POST http://localhost:8001/unload
```

### Ver Informa√ß√µes Completas

```bash
curl http://localhost:8001/info | jq
```

## üêõ Problemas Comuns

### Container n√£o inicia

```bash
# Ver logs de erro
docker-compose logs ltx2

# Verificar GPU
nvidia-smi

# Reiniciar container
docker-compose restart ltx2
```

### "Connection refused" ao chamar API

```bash
# Aguardar um pouco mais (API pode estar iniciando)
sleep 10

# Verificar se container est√° rodando
docker-compose ps

# Verificar health
curl http://localhost:8001/health
```

### Out of Memory

```bash
# Descarregar modelos n√£o usados
curl -X POST http://localhost:8001/unload
curl -X POST http://localhost:8002/unload
curl -X POST http://localhost:8003/unload

# Verificar mem√≥ria GPU
nvidia-smi
```

### V√≠deo n√£o √© gerado

```bash
# Verificar status do job
curl http://localhost:8001/jobs/SEU-JOB-ID

# Se status = "failed", verificar erro
curl http://localhost:8001/jobs/SEU-JOB-ID | jq '.error'

# Ver logs
docker-compose logs ltx2 | grep -i error
```

## üìö Pr√≥ximos Passos

Agora que est√° tudo funcionando:

1. **Explore par√¢metros**: Teste diferentes resolu√ß√µes, FPS, guidance_scale
2. **Compare modelos**: Use o mesmo prompt em diferentes modelos
3. **Benchmark**: Execute `./scripts/benchmark.py` para ver performance
4. **Customize**: Edite `.env` para configurar auto-unload, log level, etc.
5. **Leia a documenta√ß√£o**: `README.md` e `ARCHITECTURE.md` para detalhes

## üîí Auto-Unload de Modelos

Por padr√£o, modelos ficam em mem√≥ria permanentemente. Para liberar mem√≥ria automaticamente ap√≥s inatividade:

```bash
# Editar .env
nano .env

# Alterar para 30 minutos
AUTO_UNLOAD_MINUTES=30

# Reiniciar containers
docker-compose restart
```

## üìû Precisa de Ajuda?

- Verifique os logs: `docker-compose logs -f`
- Execute health check: `./scripts/health_check.py`
- Consulte `README.md` para documenta√ß√£o completa
- Consulte `ARCHITECTURE.md` para detalhes t√©cnicos

---

## Anexo: Instala√ß√£o de Depend√™ncias

### Docker

```bash
# Ubuntu/Debian
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# Adicionar usu√°rio ao grupo docker
sudo usermod -aG docker $USER
newgrp docker
```

### Docker Compose

```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install docker-compose-plugin

# Verificar
docker compose version
```

### NVIDIA Docker Runtime

```bash
# Adicionar reposit√≥rio
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list

# Instalar
sudo apt-get update
sudo apt-get install -y nvidia-docker2

# Reiniciar Docker
sudo systemctl restart docker

# Testar
docker run --rm --gpus all nvidia/cuda:12.3.0-base-ubuntu22.04 nvidia-smi
```

### HuggingFace CLI (Opcional)

```bash
pip install huggingface-hub[cli]

# Login (se necess√°rio para modelos privados)
huggingface-cli login
```

---

**Pronto!** Voc√™ est√° rodando 4 modelos de gera√ß√£o de v√≠deo no DGX Spark! üéâ
